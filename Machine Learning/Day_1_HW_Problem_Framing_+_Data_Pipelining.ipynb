{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Problem Framing:\n",
        "* Definition: breaking up the problem in hand to smaller tasks to be addressed indivisually, which in return helps to define the goals of the project and its feasibility."
      ],
      "metadata": {
        "id": "YIJ4kDUKeh_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to understand the problem:\n",
        "* Stating the goal  of the product you are developing.\n",
        "> State the goal in non-ML terms\n",
        "\n",
        "\n",
        "* Determine if the goal is best solved with ML or traditional programming skills.\n",
        ">  ML is a specialized tool suitable only for particular problems. You don't want to implement a complex ML solution when a simpler non-ML solution will work.\n",
        ">To confirm that ML is the right approach, first verify that your current non-ML solution is optimized. If you don't have a non-ML solution implemented, otherwise try solving the problem heurstically ( through trial and error).\n",
        ">>Other things to consider when comparing ML and a non-ML solution would be: Quality in terms of how much better using ML would be, and Cost and Maintenence of implementing said ML.\n",
        "\n",
        "* Verfiy the data aquired to train the model.\n",
        ">In order to use ML; the data to be used in the ML algorithm needs to meet a certain criteria:\n",
        ">>Abundance in relevency as it would affect the model's quality.\n",
        ">>Consistency and reliability as data from a reliable osurce is confrimed to produce a better model.\n",
        ">>Trusted sourced data where you know where it came from and comes from a source you have insight about.\n",
        ">>Availability of all the inputs at prediction time in all the correct format.\n",
        ">>Correctness of the lables in order to produce good predictions.\n",
        ">>Representative of the real world and accurate reflecting the events or beahvours it studies."
      ],
      "metadata": {
        "id": "nA941AnbhxF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pipelines:\n",
        "* A Machine Learning pipeline is a process of automating the\n",
        "workflow of a complete machine learning task.\n",
        "* A typical pipeline includes raw data input, features, outputs,\n",
        "model parameters, ML models, and Predictions.\n",
        "* ML Pipeline contains multiple sequential steps that perform\n",
        "everything ranging from data extraction and pre-processing to\n",
        "model training and deployment in Machine learning in a\n",
        "modular approach."
      ],
      "metadata": {
        "id": "JiMs2bKb_zVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKVP0pQfCmoz",
        "outputId": "19b369fd-ac7e-4e93-f65c-41ea242271ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2952 sha256=aecab97f3c2c9740990681fc7120b8301059385783ba526d0bc8df92b2b4bdd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X1scaCqDdIAl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple dataset\n",
        "data = {\n",
        "    'age': [25, 35, 65, 28, 42, 50, 31, 39],\n",
        "    'income': [50000, 60000, 43000, 89999, 80000, 75000, 62000, 69000],\n",
        "    'education': ['Bachelor', 'Master', 'High School', 'Master', 'PhD', 'Bachelor', 'Master', 'PhD'],\n",
        "    'target': [0, 1, 0, 1, 1, 0, 1, 0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "EJuMEVCACeIc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one-hot encoding on 'education' column\n",
        "df = pd.get_dummies(df, columns=['education'])"
      ],
      "metadata": {
        "id": "4oWaBxQoH5QU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separate features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "#Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5RyOvrrPDMpr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define preprocessing steps for numerical and categorical features\n",
        "numeric_features = ['age', 'income']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_features = ['education']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder())\n",
        "])"
      ],
      "metadata": {
        "id": "DO-qs_yjDVS7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the preprocessor using ColumnTransformer and FeatureUnion\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features)\n",
        "])\n",
        "\n",
        "#Add the feature selection step\n",
        "feature_union = FeatureUnion(transformer_list=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selector', SelectKBest(k='all'))  # k value can be adjusted based on the feature selection needs\n",
        "])"
      ],
      "metadata": {
        "id": "kS-ncHe5DhXb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the data pipeline by combining the preprocessor, feature selection, and the model\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('feature_union', feature_union),\n",
        "    ('classifier', LogisticRegression())\n",
        "])"
      ],
      "metadata": {
        "id": "vAwcRmp_Dsxs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "metadata": {
        "id": "uSms55uqDt_i"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make predictions on the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "#Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-DhjI0FDwSt",
        "outputId": "07f035b5-62f9-4103-fd32-5f5c3882822d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5\n"
          ]
        }
      ]
    }
  ]
}